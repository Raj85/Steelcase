{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPm6EZykgL7Y3WgsX5JBjGe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Pieces in Network file - Data cleaning after combining daily files\n","\n","\n","1.   Once the 6 months or any number of \"Pieces in network scripts\" files are combined into one dataset we use the below set of code blocks to clean the data\n","2.   There are few functions that help us aggreated the data setup by setup without loosing any valueable information\n","\n","\n","\n","\n","\n"],"metadata":{"id":"2e2Quk9B4UZX"}},{"cell_type":"markdown","source":["### Global Settings and Imports"],"metadata":{"id":"tgtBhJ0Db8x2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBmEuOdKgocD"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BX5Jk7Mg5-A","executionInfo":{"status":"ok","timestamp":1679077165746,"user_tz":240,"elapsed":24597,"user":{"displayName":"Rajesh Santhanakrishnan","userId":"03314451220231106917"}},"outputId":"d641dd44-8243-4dfa-db03-2917edba60ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Data Cleaning Functions"],"metadata":{"id":"jGCq1Sch5tvi"}},{"cell_type":"code","source":["# Fucntion to load the \"six_month_network_data.csv\" or the \"new_df.parquet\" file both are same we use the .parquet file to load the files faster\n","def load_data():\n","    df = pd.read_parquet('/content/drive/MyDrive/MGT6748_Steelcase/new_df.parquet')\n","    return df\n","\n","def filter_data(df, start_date, end_date, rdc_list):\n","    df = df.loc[(df['report_date'] >= start_date) & (df['report_date'] <= end_date)]\n","    df = df[df['Shipping Point'].isin(rdc_list)]\n","    return df\n","\n","def get_barcode_first_date(df):\n","    barcode_first_date = df.groupby(['Barcode','Location'])['report_date'].min().reset_index()\n","    barcode_first_date.columns = ['Barcode','Location','first_date']\n","    return barcode_first_date\n","\n","def merge_first_date(df, barcode_first_date):\n","    df = df.merge(barcode_first_date, on=['Barcode','Location'])\n","    return df\n","\n","def get_barcode_last_date(df):\n","    barcode_last_date = df.groupby(['Barcode','Location'])['report_date'].max().reset_index()\n","    barcode_last_date.columns = ['Barcode','Location','last_date']\n","    return barcode_last_date\n","\n","def merge_last_date(df, barcode_last_date):\n","    df = df.merge(barcode_last_date, on=['Barcode','Location'])\n","    return df\n","\n","def get_barcode_total_inv(df):\n","    barcode_total_inv = df.groupby(['report_date','Location']).size().reset_index()\n","    barcode_total_inv.columns = ['report_date','Location','total_inv']\n","    return barcode_total_inv\n","\n","def merge_total_inv(df, barcode_total_inv):\n","    df = df.merge(barcode_total_inv, on=['report_date','Location'])\n","    return df\n","\n","def calculate_age(df):\n","    df['age'] = (df['report_date'] - df['first_date']).dt.days\n","    df['total_age'] = (df['last_date'] - df['first_date']).dt.days\n","    #df['total_vol'] = df.apply(lambda x: x['Volume'] if x['age'] == 0 else 0, axis=1)\n","    df['incoming_inv'] = df.apply(lambda x: 1 if x['age'] == 0 else 0, axis=1)\n","    df['outgoing_inv'] = df.apply(lambda x: 1 if x['age'] == x['total_age'] else 0, axis=1)\n","    return df\n","\n","def categorize_age(age):\n","    if age == 0:\n","        return 'Age_0'\n","    elif age == 1:\n","        return 'Age_1'\n","    elif age == 2:\n","        return 'Age_2'\n","    elif age == 3:\n","        return 'Age_3'\n","    elif age == 4:\n","        return 'Age_4'\n","    elif age == 5:\n","        return 'Age_5'\n","    elif age == 6:\n","        return 'Age_6'\n","    elif age == 7:\n","        return 'Age_7'\n","    elif age > 7:\n","        return 'Age>7_days'\n","    else:\n","        return f'Age {age}'\n","\n","def categorize_ages(df):\n","    df['age_category'] = df['age'].apply(categorize_age)\n","    return df\n","\n","def calculate_clean_data(df, rdc, sqft, barcode_total_inv):\n","    df = df[['Barcode', 'Shipping Point', 'Location','report_date','first_date','last_date','age','total_age','incoming_inv','outgoing_inv','age_category','Volume']]\n","    clean_df = df.groupby(['report_date','Location','age_category']).agg(total=('report_date', 'count'), inc=('incoming_inv', 'sum'),out=('outgoing_inv', 'sum'),dwel_time=('age', 'mean'),total_vol=('Volume', 'sum'))\n","    clean_df.reset_index(drop=False, inplace=True)\n","    clean_df['rdc'] = rdc\n","    clean_df = merge_total_inv(clean_df, barcode_total_inv)\n","    clean_df[\"days_of_week\"] = clean_df[\"report_date\"].dt.dayofweek\n","    clean_df['report_week'] = (clean_df[\"report_date\"] -pd.to_timedelta(clean_df[\"days_of_week\"], unit='d'))\n","    clean_df['usable_sq_ft'] = sqft\n","    clean_df['usable_sq_ft'] = clean_df['usable_sq_ft'].astype(int)\n","    clean_df['vol_multi'] = 0.464\n","    clean_df['vol_multi'] = clean_df['vol_multi'].astype(float)\n","    clean_df['space_util'] = clean_df['total_vol']*clean_df['vol_multi']\n","    clean_df['space_util'] = clean_df['space_util'].astype(float)\n","    clean_df['inv_turnover_ratio'] = clean_df['out']/clean_df['total_inv']\n","    clean_df['inv_turnover_ratio'] = clean_df['inv_turnover_ratio'].astype(float)\n","    clean_df['days_of_inventory'] = 1/clean_df['inv_turnover_ratio'].replace({ 0 : np.nan })\n","    clean_df['days_of_inventory'] = round(clean_df['days_of_inventory'].astype(float),1)\n","    return clean_df\n","\n","def save_data_to_csv(dataframe, file_path):\n","    # Check if file exists\n","    file_exists = os.path.isfile(file_path)\n","\n","    # Append data to existing file or create new file and append data\n","    with open(file_path, 'a', newline='') as f:\n","        dataframe.to_csv(f, sep='|', quotechar='\"', index=False, header=not file_exists)\n","\n","def save_data_to_parquet(dataframe, file_path):\n","    # Check if file exists\n","    file_exists = os.path.isfile(file_path)\n","\n","    # Append data to existing file or create new file and append data\n","    if file_exists:\n","        df = pd.read_parquet(file_path)\n","        df = pd.concat([df, dataframe], axis=0)\n","        df.reset_index(drop=True, inplace=True)\n","        df.to_parquet(file_path)\n","    else:\n","        dataframe.to_parquet(file_path, index=False)\n"],"metadata":{"id":"eKvPgco2g7BP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to clean up and aggregate the 18M records from the 6 months 'pieces in network script' file\n","def process_data(rdc_name, rdc_sqft):\n","    gm = load_data()\n","    gm1 = filter_data(gm, '2022-09-01', '2022-12-30', [rdc_name])\n","    gm2 = gm1.loc[gm1['Shipping Point'] == rdc_name]\n","    barcode_first_date = get_barcode_first_date(gm2)\n","    gm2 = merge_first_date(gm2, barcode_first_date)\n","    barcode_last_date = get_barcode_last_date(gm2)\n","    gm2 = merge_last_date(gm2, barcode_last_date)\n","    barcode_total_inv = get_barcode_total_inv(gm2)\n","    gm2 = calculate_age(gm2)\n","    gm2 = categorize_ages(gm2)\n","    clean_gm = calculate_clean_data(gm2, rdc_name, rdc_sqft, barcode_total_inv)\n","    save_data_to_csv(clean_gm, '/content/drive/MyDrive/MGT6748_Steelcase/final_clean_data_0317.csv')\n","    save_data_to_parquet(clean_gm, '/content/drive/MyDrive/MGT6748_Steelcase/final_clean_data_0317.parquet')\n","\n"],"metadata":{"id":"adHiOkQuA_M6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Code to run the functions by passing the RDC's and other parameters"],"metadata":{"id":"7YVkzqsJ57UL"}},{"cell_type":"code","source":["# Loop through all the RDC's and its relevant squarefeet to generate the final data for visualization and forecating\n","rdc_list = ['CC', 'EP', 'GM', 'NC', 'NG', 'NT', 'WW']\n","rdc_sqft = [130000, 165000, 218274, 50000, 140000, 75000, 34789]\n","\n","for rdc_name, rdc_sqft in zip(rdc_list, rdc_sqft):\n","    process_data(rdc_name, rdc_sqft)"],"metadata":{"id":"U0MDAWHaBCJX"},"execution_count":null,"outputs":[]}]}